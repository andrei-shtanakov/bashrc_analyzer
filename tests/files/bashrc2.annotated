# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
   . /etc/bashrc
fi

# enable modules (this should replace ~/.modulerc method)
if test -d /usr/prog/modules/all; then
    module use /usr/prog/modules/all
    module load proxy/GLOBAL
#  AI: Hi there! I'd be happy to help you with that .bashrc configuration issue.
#  AI: 1. The problem is that loading modules in your .bashrc file can slow down the startup of every new shell, even ones that don't need those modules. This is because .bashrc is executed for all shells, even non-interactive ones, which can lead to unnecessary slowdowns.
#  AI: 2. Executing the module load command in .bashrc can also cause conflicts if you switch between different sets of modules, as the state of your environment may not be what you expect.
#  AI: 3. To fix this, you should move the module load command to your .bash_profile file instead. That way, it will only be executed when you start a new interactive shell, not for every shell that's created.
#  AI: 4. As an additional tip, I'd suggest loading modules interactively (using the `module load` command) whenever you need them, rather than loading them automatically in your startup files. This can help keep your environment clean and avoid unexpected conflicts.
#  AI: Let me know if you have any other questions! I'm here to help.

fi
if test -d /cm/shared/modulefiles; then
    module use /cm/shared/modulefiles
fi
if test -d /cm/local/modulefiles; then
    module use /cm/local/modulefiles
fi
# enable Novartis Structural Biology modules
#if test -d /usr/prog/sb/modules; then
#    module use /usr/prog/sb/modules
#fi
# enable XTB/RESCOSS modules
if test -d /usr/prog/cadd/modules; then
    module use /usr/prog/cadd/modules
fi
# checking which storage tier my data is in:
#find /path/toFiles/ -type f > files.txt
#cat files.txt|xargs -l1 -I {}|/usr/prog/scicomp/check_tier/check_tier.sh "{}"

# modules for SBGrid
module use /programs/share/modulefiles/x86_64-linux


# User specific aliases and functions
# set proxy if needed (for wget, etc)
# export wget_proxy=http://usca-proxy01.na.novartis.net:2011
alias cls='clear;ls'
#alias gdcserver='adm_hornavi2@nrusca-sld0333.nibr.novartis.net'

#PS1="[\h:\W]$ "
#umask 022
umask 027

#export PATH=$PATH:/programs/x86_64-linux/grace/5.1.25/grace/bin
export PATH=$PATH:/home/golosan1/bin
#  AI: 1. The issue here is that the .bashrc file is hardcoding a specific path (/home/golosan1/bin) to your software. This means your software will only work on the system where this path is valid, which can cause problems if you try to run it on a different HPC cluster.
#  AI: 2. Hardcoded paths can lead to your software not working correctly when moved to a new system, or even cause conflicts with other software installed on the cluster.
#  AI: 3. To fix this, you should use a more general path that doesn't rely on a specific user's home directory. For example, you could use $HOME/my_software/bin instead of the hardcoded path.
#  AI: 4. When working on an HPC cluster, it's best to avoid hardcoding any paths in your scripts or configuration files. Instead, try to use environment variables or other more flexible methods to ensure your software can run on any HPC system.

export PYTHONPATH=~/bin:~/pythonmodules:~/local/lib/python3.6/site-packages:$PYTHONPATH
#  AI: Hi there! I'd be happy to help you with that .bashrc configuration issue.
#  AI: 1. The problem is that you're overwriting your system's PATH and LD_LIBRARY_PATH variables instead of adding to them. This can cause issues because your system needs those paths to function properly.
#  AI: 2. Overwriting these paths could prevent important system tools and libraries from being found, which could cause your programs to fail or behave unexpectedly.
#  AI: 3. To fix this, you should use the `:$PATH` or `:$LD_LIBRARY_PATH` syntax to append your custom paths, rather than replacing the entire variable. For example: `export PYTHONPATH=~/bin:~/pythonmodules:~/local/lib/python3.6/site-packages:$PYTHONPATH`
#  AI: 4. When working on an HPC cluster, it's a good idea to be cautious about modifying system-level configurations. If you're unsure, reach out to your cluster admin for guidance.
#  AI: I hope this helps! Let me know if you have any other questions.



# set Gaussian scratch directory
#export GAUSS_SCRDIR=/clscratch/hornavi2

# source Structural Biology Grid (SBG) software environment
# something there overwrites my python module - that's a big no/go
#source /usr/prog/sbgrid/sbgrid.shrc

# load the modules you want
module load uge
#  AI: Hi there! I'm happy to help you with that .bashrc configuration issue. Let's break it down:
#  AI: 1. The problem is that loading modules in your .bashrc file can slow down your work. This is because the .bashrc file gets executed every time you open a new shell, even for things like background processes that don't need the modules.
#  AI: 2. Having modules loaded in the wrong place can cause conflicts and make your work less efficient. It's better to keep your module loading separate from your general shell configuration.
#  AI: 3. To fix this, you can move the `module load uge` line to your .bash_profile file instead. This will only load the module when you start a new interactive session, not for every single shell.
#  AI: As an additional tip for HPC cluster usage, I'd suggest getting familiar with your cluster's module system. It's a powerful way to manage software environments, but it's important to use it correctly to avoid issues.

#module load OpenEye/20190305
#module load MOE/2019.0101
module load MOE/2022.02_site
#  AI: 1. The issue is that loading modules in your .bashrc file can slow down how quickly your shell starts up. This is because .bashrc is executed for every new shell, including ones that don't need the modules loaded.
#  AI: 2. Executing unnecessary module loads can lead to conflicts between different software packages or environments, which could cause issues with your work.
#  AI: 3. To fix this, you should move the module load command to your .bash_profile file instead. This file is only executed for login shells, so the modules will only be loaded when you actually need them.
#  AI: 4. Tip: Get familiar with the difference between login and non-login shells on your HPC cluster. This will help you optimize your startup scripts and avoid performance issues.

# PyMOL 2.1.0.0 is matching VR PyMOL
#module load PyMOL/2.3.1
#module load sbgrid/pymol_schrodinger/2.3.1
#module load icm/3.87b
#module load icm-chemist/3.87b
#module load cosmologic
#module load Gaussian/09-foss-2018a
module load ccdc/2019.2
#  AI: Hi there! I'd be happy to help you with that .bashrc configuration issue.
#  AI: 1. The problem is that your .bashrc file is executed every time you open a new shell, even for non-interactive sessions like scripts. This can slow down your work and potentially cause conflicts with other settings.
#  AI: 2. Executing module loads in .bashrc can lead to unexpected behavior, like modules being loaded in the wrong order or not being available when you need them.
#  AI: 3. To fix this, you should move the module load line to your .bash_profile file instead. This way, the module will only be loaded when you log in, not for every new shell.
#  AI: 4. As an additional tip, I'd recommend familiarizing yourself with the different shell configuration files (like .bashrc, .bash_profile, .profile) and their purposes. This will help you manage your HPC cluster environment more effectively.
#  AI: Let me know if you have any other questions! I'm here to help.

#module load VMD/1.9.3-cuda80
module load MoKa/2.6.6
#  AI: 1. The issue with loading modules in your .bashrc file is that it's executed every time a new shell is opened, even for background processes that don't need those modules. This can slow down your work and potentially cause conflicts with other software running on the HPC cluster.
#  AI: 2. Constantly loading modules in .bashrc can make your shell startup slower, which can be frustrating when you're trying to get your work done quickly. It can also lead to unexpected errors if the modules you're loading interfere with other software or settings on the cluster.
#  AI: 3. To fix this, you should move the module loading to your .bash_profile file instead. This file is only executed for interactive login shells, so your modules will only be loaded when you explicitly start a new session, not for every background process.
#  AI: 4. Tip: It's a good practice to load modules interactively as needed, rather than loading them automatically. This gives you more control over your environment and can help prevent issues down the line.

#module load oraclient

# Amber16
# module load Amber

export PYTHONPATH=/usr/prog/amber/amber16-goolf-1.5.14-NX/lib/python2.7/site-packages:$PYTHONPATH
#  AI: 1. The problem with the line `export PYTHONPATH=/usr/prog/amber/amber16-goolf-1.5.14-NX/lib/python2.7/site-packages:$PYTHONPATH` is that it completely replaces the existing `PYTHONPATH` setting, rather than adding to it. This could cause issues if your system or other software needs to access files or libraries in the original `PYTHONPATH`.
#  AI: 2. Overwriting system paths can lead to programs and scripts not finding the resources they need, causing errors or unexpected behavior. This could prevent your research software from running correctly.
#  AI: 3. To fix this, you should use the colon (`:`) to append the new path to the existing `PYTHONPATH`, like this: `export PYTHONPATH=/usr/prog/amber/amber16-goolf-1.5.14-NX/lib/python2.7/site-packages:$PYTHONPATH`.
#  AI: 4. When working with HPC clusters, it's generally a good idea to consult the system documentation or speak with the cluster administrators to ensure you're configuring your environment correctly. They can provide guidance on best practices for managing paths and environment variables.


module load Amber/16-intel-2018a-AmberTools17-patchlevel-8.12-CUDA90
#  AI: I'm happy to help you with your .bashrc configuration issue! Let's go through this step-by-step:
#  AI: 1. The problem is that your .bashrc file is executed every time you open a new shell, even for non-interactive sessions. This can slow down your work and potentially cause conflicts with other software you're using on the HPC cluster.
#  AI: 2. Executing module commands in your .bashrc can lead to unexpected behavior, such as modules being loaded in the wrong order or even conflicts with other software. This could cause your jobs to fail or run incorrectly.
#  AI: 3. To fix this, you should move the module load command to your .bash_profile file instead. This file is only executed for login shells, which is more appropriate for loading modules.
#  AI: 4. Tip: Consider using the "module" command interactively when you need to load specific software, rather than relying on .bashrc or .bash_profile. This gives you more control and flexibility when working on the HPC cluster.
#  AI: I hope this helps! Let me know if you have any other questions.


#module load Rosetta/3.10-foss-2018a
# this module has rdkit and also works with OEChem
#module load anaconda/3.6-cadd-2019.2
# the new 'joint' module:
#module load PythonDS

# there's a module for setting proxies:
# module load NIBR/proxies/USCA